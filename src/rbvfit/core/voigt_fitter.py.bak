"""
Enhanced fitter class for rbvfit 2.0 with multi-group and multi-instrument support.

This module provides unified MCMC fitting capabilities for both single and multiple
datasets with shared physics parameters but different instrumental responses.
"""

from __future__ import annotations
from typing import Dict, List, Tuple, Optional, Any, Union, Callable
import numpy as np
from dataclasses import dataclass
import warnings
from multiprocessing import Pool
import time

# MCMC backends
import emcee
try:
    import zeus
    HAS_ZEUS = True
except ImportError:
    HAS_ZEUS = False
    zeus = None

# Core rbvfit 2.0 imports
from rbvfit.core.fit_configuration import FitConfiguration
from rbvfit.core.parameter_manager import ParameterManager, ParameterBounds
from rbvfit.core.voigt_model import VoigtModel


@dataclass
class Dataset:
    """
    Container for a single dataset with its instrumental parameters.
    
    Attributes
    ----------
    wavelength : np.ndarray
        Wavelength array in Angstroms
    flux : np.ndarray
        Normalized flux array
    error : np.ndarray
        Error array for flux
    name : str, optional
        Dataset identifier
    lsf_params : dict, optional
        Line spread function parameters specific to this dataset
    """
    wavelength: np.ndarray
    flux: np.ndarray
    error: np.ndarray
    name: str = "dataset"
    lsf_params: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        """Validate dataset consistency."""
        if not (len(self.wavelength) == len(self.flux) == len(self.error)):
            raise ValueError("Wavelength, flux, and error arrays must have same length")
        if np.any(self.error <= 0):
            raise ValueError("Error array must contain only positive values")
        if not np.all(np.isfinite(self.wavelength)):
            raise ValueError("Wavelength array contains non-finite values")
        if not np.all(np.isfinite(self.flux)):
            raise ValueError("Flux array contains non-finite values")
        if not np.all(np.isfinite(self.error)):
            raise ValueError("Error array contains non-finite values")


@dataclass
class MCMCSettings:
    """
    MCMC configuration parameters.
    
    Attributes
    ----------
    sampler : str
        MCMC sampler backend ('emcee' or 'zeus')
    n_walkers : int
        Number of MCMC walkers
    n_steps : int
        Number of MCMC steps
    n_burn : int, optional
        Number of burn-in steps (default: 20% of n_steps)
    thin : int
        Thinning factor for samples
    parallel : bool
        Whether to use parallel processing
    progress : bool
        Whether to show progress bar
    """
    sampler: str = 'emcee'
    n_walkers: int = 50
    n_steps: int = 1000
    n_burn: Optional[int] = None
    thin: int = 1
    parallel: bool = True
    progress: bool = True
    
    def __post_init__(self):
        """Validate MCMC settings."""
        if self.sampler not in ['emcee', 'zeus']:
            raise ValueError(f"Unknown sampler: {self.sampler}. Use 'emcee' or 'zeus'")
        if self.sampler == 'zeus' and not HAS_ZEUS:
            raise ImportError("Zeus sampler requested but not installed. Install with: pip install zeus-mcmc")
        if self.n_burn is None:
            self.n_burn = max(100, int(0.2 * self.n_steps))
        if self.n_walkers < 2:
            raise ValueError("Number of walkers must be >= 2")
        if self.n_steps < 1:
            raise ValueError("Number of steps must be >= 1")


class VoigtFitter:
    """
    Main fitter class for single or multiple datasets.
    
    This class handles MCMC fitting with automatic parameter management,
    bounds generation, and support for multiple MCMC backends.
    
    Parameters
    ----------
    model : VoigtModel
        The Voigt model to fit
    datasets : Dataset or List[Dataset]
        Single dataset or list of datasets to fit
    mcmc_settings : MCMCSettings, optional
        MCMC configuration parameters
        
    Examples
    --------
    >>> config = FitConfiguration()
    >>> config.add_system(0.348, 'MgII', [2796.3, 2803.5], components=2)
    >>> model = VoigtModel(config)
    >>> dataset = Dataset(wave, flux, error)
    >>> fitter = VoigtFitter(model, dataset)
    >>> result = fitter.fit(initial_guess, bounds)
    """
    
    def __init__(self, model: VoigtModel, datasets: Union[Dataset, List[Dataset]], 
                 mcmc_settings: Optional[MCMCSettings] = None):
        """Initialize the fitter."""
        self.model = model
        self.param_manager = model.param_manager
        # Compile model for fast MCMC evaluation

        
        # Handle single dataset vs multiple datasets
        if isinstance(datasets, Dataset):
            self.datasets = [datasets]
            self.is_joint_fit = False
        else:
            self.datasets = datasets
            self.is_joint_fit = len(datasets) > 1
            
        # Validate datasets
        for i, dataset in enumerate(self.datasets):
            if not isinstance(dataset, Dataset):
                raise TypeError(f"Dataset {i} is not a Dataset object")
                
        # Set up MCMC settings
        self.mcmc_settings = mcmc_settings or MCMCSettings()
        
        # Get parameter structure
        self.structure = self.param_manager.config_to_theta_structure()
        self.n_params = self.structure['total_parameters']
        
        # Initialize sampler-related attributes
        self.sampler = None
        self.initial_state = None
        
    def _log_prior(self, theta: np.ndarray, bounds: ParameterBounds) -> float:
        """
        Calculate log prior probability.
        
        Parameters
        ----------
        theta : np.ndarray
            Parameter array
        bounds : ParameterBounds
            Parameter bounds
            
        Returns
        -------
        float
            Log prior probability
        """
        # Uniform prior within bounds
        if np.any(theta < bounds.lower) or np.any(theta > bounds.upper):
            return -np.inf
        return 0.0
    
    def _log_likelihood(self, theta: np.ndarray) -> float:
        """
        Calculate log likelihood for all datasets.
        
        Parameters
        ----------
        theta : np.ndarray
            Parameter array
            
        Returns
        -------
        float
            Log likelihood
        """
        #total_log_likelihood = 0.0
        dataset = self.datasets[0]
        
        #model_dat = self.model.evaluate(theta, dataset.wavelength, validate_theta=False)
        model_flux = self.model.model_flux(theta, dataset.wavelength)
        inv_sigma2 = 1.0 / (dataset.error ** 2)
        lnlike_total = -0.5 * (np.sum((dataset.flux - model_dat) ** 2 * inv_sigma2 - np.log(inv_sigma2)))


        #for dataset in self.datasets:
        #    # Use model's default LSF
        #    model_flux = self.model.evaluate(theta, dataset.wavelength)#
        #        
        #    # Calculate chi-squared
        #    chi2 = np.sum((dataset.flux - model_flux)**2 / dataset.error**2)
        #    
        #    # Add to total log likelihood
        #    total_log_likelihood += -0.5 * chi2
            
        return lnlike_total
    
    def _log_probability(self, theta: np.ndarray) -> float:  # No bounds argument
        """
        Calculate log posterior probability.
        
        Parameters
        ----------
        theta : np.ndarray
            Parameter array
        bounds : ParameterBounds
            Parameter bounds
            
        Returns
        -------
        float
            Log posterior probability
        """
        # Check prior first (cheaper)
        log_prior = self._log_prior(theta, self.bounds)  # Use stored bounds
        if not np.isfinite(log_prior):
            return -np.inf
            
        # Calculate likelihood
        try:
            log_likelihood = self._log_likelihood(theta)
            if not np.isfinite(log_likelihood):
                return -np.inf
            return log_prior + log_likelihood
        except Exception:
            # Return -inf for any evaluation errors
            return -np.inf
    
    def _initialize_walkers(self, initial_guess: np.ndarray, bounds: ParameterBounds, 
                           perturbation: float = 1e-4) -> np.ndarray:
        """
        Initialize walker positions.
        
        Parameters
        ----------
        initial_guess : np.ndarray
            Initial parameter guess
        bounds : ParameterBounds
            Parameter bounds
        perturbation : float
            Perturbation scale for walker initialization
            
        Returns
        -------
        np.ndarray
            Walker positions (n_walkers, n_params)
        """
        n_walkers = self.mcmc_settings.n_walkers
        n_params = len(initial_guess)
        
        # Create walker positions with small perturbations
        positions = np.zeros((n_walkers, n_params))
        
        for i in range(n_walkers):
            attempts = 0
            max_attempts = 1000
            
            while attempts < max_attempts:
                # Add random perturbation
                pos = initial_guess + perturbation * np.random.randn(n_params)
                
                # Ensure within bounds
                pos = np.clip(pos, bounds.lower + 1e-10, bounds.upper - 1e-10)
                
                # Check if position is valid
                if np.isfinite(self._log_probability(pos)):
                    positions[i] = pos
                    break
                    
                attempts += 1
                
            if attempts >= max_attempts:
                raise RuntimeError(f"Could not initialize walker {i} after {max_attempts} attempts")
                
        return positions
    
    def fit(self, initial_guess: np.ndarray, bounds: Optional[ParameterBounds] = None,
            optimize_first: bool = True) -> 'FitResults':
        """
        Perform MCMC fit.
        
        Parameters
        ----------
        initial_guess : np.ndarray
            Initial parameter guess
        bounds : ParameterBounds, optional
            Parameter bounds (auto-generated if not provided)
        optimize_first : bool
            Whether to optimize initial guess before MCMC
            
        Returns
        -------
        FitResults
            Fitting results object
        """
        # Validate initial guess
        if len(initial_guess) != self.n_params:
            raise ValueError(f"Initial guess length ({len(initial_guess)}) doesn't match "
                           f"expected parameters ({self.n_params})")
        
        # Generate bounds if not provided
        if bounds is None:
            bounds = self.param_manager.generate_parameter_bounds()
        self.bounds = bounds  # Store bounds in object

            
        # Optimize initial guess if requested
        if optimize_first:
            print("Optimizing initial guess...")
            optimized_guess = self._optimize_guess(initial_guess, bounds)
            print("Optimization complete.")
        else:
            optimized_guess = initial_guess.copy()
            
        # Set up MCMC sampler
        print(f"Setting up {self.mcmc_settings.sampler} sampler...")
        if self.mcmc_settings.sampler == 'emcee':
            self.sampler = self._setup_emcee_sampler()
        elif self.mcmc_settings.sampler == 'zeus':
            self.sampler = self._setup_zeus_sampler()
        else:
            raise ValueError(f"Unknown sampler: {self.mcmc_settings.sampler}")
            
        # Initialize walkers
        print("Initializing walkers...")
        initial_positions = self._initialize_walkers(optimized_guess, bounds)
        
        # Run MCMC
        print(f"Running MCMC with {self.mcmc_settings.n_walkers} walkers "
              f"for {self.mcmc_settings.n_steps} steps...")
        
        start_time = time.time()
        
        if self.mcmc_settings.sampler == 'emcee':
            self.sampler.run_mcmc(initial_positions, self.mcmc_settings.n_steps, 
                                 progress=self.mcmc_settings.progress)
        elif self.mcmc_settings.sampler == 'zeus':
            self.sampler.run_mcmc(initial_positions, self.mcmc_settings.n_steps,
                                 progress=self.mcmc_settings.progress)
            
        end_time = time.time()
        print(f"MCMC completed in {end_time - start_time:.1f} seconds")
        
        # Create results object
        return FitResults(
            sampler=self.sampler,
            param_manager=self.param_manager,
            datasets=self.datasets,
            mcmc_settings=self.mcmc_settings,
            bounds=bounds,
            model=self.model,
            fit_time=end_time - start_time
        )
    
    def _optimize_guess(self, initial_guess: np.ndarray, bounds: ParameterBounds) -> np.ndarray:
        """
        Optimize initial guess using scipy.optimize.
        
        Parameters
        ----------
        initial_guess : np.ndarray
            Initial parameter guess
        bounds : ParameterBounds
            Parameter bounds
            
        Returns
        -------
        np.ndarray
            Optimized parameter guess
        """
        from scipy.optimize import minimize
        
        # Define objective function (negative log likelihood)
        def objective(theta):
            return -self._log_likelihood(theta)
            
        # Set up bounds for scipy
        scipy_bounds = list(zip(bounds.lower, bounds.upper))
        
        # Run optimization
        try:
            result = minimize(objective, initial_guess, bounds=scipy_bounds, method='L-BFGS-B')
            if result.success:
                return result.x
            else:
                warnings.warn(f"Optimization failed: {result.message}. Using original guess.")
                return initial_guess
        except Exception as e:
            warnings.warn(f"Optimization error: {e}. Using original guess.")
            return initial_guess
    
    def _setup_emcee_sampler(self):
        """Set up emcee sampler."""
        if self.mcmc_settings.parallel:
            pool = Pool()
            sampler = emcee.EnsembleSampler(
                self.mcmc_settings.n_walkers, 
                self.n_params,
                self._log_probability,
                pool=pool
            )
        else:
            sampler = emcee.EnsembleSampler(
                self.mcmc_settings.n_walkers,
                self.n_params, 
                self._log_probability,
            )
        return sampler
    
    def _setup_zeus_sampler(self):
        """Set up zeus sampler."""
        if not HAS_ZEUS:
            raise ImportError("Zeus sampler not available")
            
        if self.mcmc_settings.parallel:
            pool = Pool()
            sampler = zeus.EnsembleSampler(
                self.mcmc_settings.n_walkers,
                self.n_params,
                self._log_probability,
                pool=pool
            )
        else:
            sampler = zeus.EnsembleSampler(
                self.mcmc_settings.n_walkers,
                self.n_params,
                self._log_probability, 
            )
        return sampler


class JointFitter(VoigtFitter):
    """
    Specialized fitter for multiple datasets with shared physics parameters.
    
    This class extends VoigtFitter to handle multiple datasets that share
    the same physical absorption parameters but may have different instrumental
    responses (LSF, resolution, etc.).
    
    Examples
    --------
    >>> # Different instruments observing same absorption system
    >>> dataset_cos = Dataset(wave_cos, flux_cos, error_cos, 
    ...                       lsf_params={'FWHM': 'COS', 'grating': 'G130M'})
    >>> dataset_stis = Dataset(wave_stis, flux_stis, error_stis,
    ...                        lsf_params={'FWHM': '3.0'})
    >>> fitter = JointFitter(model, [dataset_cos, dataset_stis])
    >>> result = fitter.fit(initial_guess)
    """
    
    def __init__(self, model: VoigtModel, datasets: List[Dataset],
                 mcmc_settings: Optional[MCMCSettings] = None):
        """Initialize joint fitter."""
        if not isinstance(datasets, list) or len(datasets) < 2:
            raise ValueError("JointFitter requires at least 2 datasets")
            
        super().__init__(model, datasets, mcmc_settings)
        self.is_joint_fit = True
        
        # Validate that datasets have compatible wavelength coverage
        self._validate_wavelength_coverage()
        
    def _validate_wavelength_coverage(self):
        """Validate that datasets have reasonable wavelength coverage for joint fitting."""
        # Check for significant wavelength overlap or complementary coverage
        wave_ranges = [(d.wavelength.min(), d.wavelength.max()) for d in self.datasets]
        
        # Log dataset information
        print(f"Joint fitting {len(self.datasets)} datasets:")
        for i, (dataset, (wmin, wmax)) in enumerate(zip(self.datasets, wave_ranges)):
            print(f"  Dataset {i+1} ({dataset.name}): {wmin:.1f} - {wmax:.1f} Ã…")
            
        # Additional validation could be added here for specific requirements


# Placeholder for FitResults class (to be implemented in Step 2.2)
class FitResults:
    """
    Container for MCMC fitting results.
    
    This is a placeholder implementation that will be fully developed
    in Phase 2, Step 2.2: Results Management.
    """
    
    def __init__(self, sampler, param_manager, datasets, mcmc_settings, bounds, model, fit_time):
        """Initialize results container."""
        self.sampler = sampler
        self.param_manager = param_manager
        self.datasets = datasets
        self.mcmc_settings = mcmc_settings
        self.bounds = bounds
        self.model = model
        self.fit_time = fit_time
        
        # Extract basic results
        self._extract_basic_results()
        
    def _extract_basic_results(self):
        """Extract basic results from sampler."""
        # Get samples (discard burn-in)
        if hasattr(self.sampler, 'get_chain'):  # emcee
            self.samples = self.sampler.get_chain(
                discard=self.mcmc_settings.n_burn,
                thin=self.mcmc_settings.thin,
                flat=True
            )
        else:  # zeus
            chain = self.sampler.get_chain(flat=True)
            n_total = len(chain)
            start_idx = self.mcmc_settings.n_burn * self.mcmc_settings.n_walkers
            self.samples = chain[start_idx::self.mcmc_settings.thin]
            
        # Calculate basic statistics
        self.best_fit = np.median(self.samples, axis=0)
        self.uncertainties = np.std(self.samples, axis=0)
        
        # Calculate parameter percentiles
        self.percentiles = {}
        for p in [16, 50, 84]:
            self.percentiles[p] = np.percentile(self.samples, p, axis=0)
            
    def summary(self) -> str:
        """Generate a summary of fit results."""
        lines = ["MCMC Fit Results", "=" * 50]
        lines.append(f"Sampler: {self.mcmc_settings.sampler}")
        lines.append(f"Walkers: {self.mcmc_settings.n_walkers}")
        lines.append(f"Steps: {self.mcmc_settings.n_steps}")
        lines.append(f"Burn-in: {self.mcmc_settings.n_burn}")
        lines.append(f"Samples: {len(self.samples)}")
        lines.append(f"Fit time: {self.fit_time:.1f} seconds")
        lines.append(f"Datasets: {len(self.datasets)}")
        
        # Add parameter summary
        param_names = self.param_manager.get_parameter_names()
        lines.append("\nParameter Summary:")
        lines.append("-" * 40)
        
        for i, name in enumerate(param_names):
            best = self.best_fit[i]
            lower = self.best_fit[i] - self.percentiles[16][i]
            upper = self.percentiles[84][i] - self.best_fit[i]
            lines.append(f"{name}: {best:.3f} +{upper:.3f} -{lower:.3f}")
            
        return "\n".join(lines)